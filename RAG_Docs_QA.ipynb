{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpaUMDWADIbezWSBJ7o95B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaysahoo03/RAG_Document_QA/blob/main/RAG_Docs_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94fBibuKQUIT"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq langchain sentence-transformers faiss-cpu pypdf\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata, files\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import Document\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Groq Setup\n",
        "groq_api = userdata.get('groq_api')\n",
        "llm = ChatGroq(api_key=groq_api, model=\"gemma2-9b-it\", temperature=0.1)\n",
        "\n",
        "# Embeddings (free Hugging Face model)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"Setup complete!\")"
      ],
      "metadata": {
        "id": "kjpdBfdcQXE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load docs (PDF or text)\n",
        "def load_documents(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "    else:\n",
        "        loader = TextLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "# Sample: Upload a file (run this to upload)\n",
        "uploaded = files.upload()  # Upload a PDF or TXT (e.g., INFOSYS -APTITUDE-MODEL PAPERS_1473177928759.pdf)\n",
        "file_name = list(uploaded.keys())[0]  # Get first uploaded file\n",
        "docs = load_documents(file_name)\n",
        "print(f\"Loaded {len(docs)} documents.\")\n",
        "\n",
        "# Split into chunks and clean\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Filter out empty or invalid chunks\n",
        "cleaned_splits = [doc for doc in splits if doc.page_content and doc.page_content.strip()]\n",
        "print(f\"Split into {len(cleaned_splits)} valid chunks after cleaning.\")"
      ],
      "metadata": {
        "id": "SL2aPmpgQW7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Save index (optional, for reuse)\n",
        "vectorstore.save_local(\"faiss_index\")\n",
        "\n",
        "print(\"Vector store built! Ready for retrieval.\")"
      ],
      "metadata": {
        "id": "D42N_mOTQW1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template for Q&A with citations\n",
        "prompt_template = \"\"\"\n",
        "Use the following context to answer the question. If you don't know, say so. Cite sources with [source chunk index].\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Retrieval QA Chain (top-3 chunks)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    chain_type_kwargs={\"prompt\": PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"RAG chain ready!\")"
      ],
      "metadata": {
        "id": "5QPK4-tzQWy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive loop\n",
        "print(\"RAG Q&A System Ready! Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nQuestion: \")\n",
        "    if query.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Run query\n",
        "    result = qa_chain({\"query\": query})\n",
        "    answer = result['result']\n",
        "    sources = result['source_documents']\n",
        "\n",
        "    print(f\"\\nAnswer: {answer}\")\n",
        "    print(\"\\nSources:\")\n",
        "    for i, doc in enumerate(sources):\n",
        "        print(f\"[Chunk {i+1}]: {doc.page_content[:200]}... (Page: {getattr(doc.metadata, 'page', 'N/A')})\")"
      ],
      "metadata": {
        "id": "SD3TN4w_QWw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}